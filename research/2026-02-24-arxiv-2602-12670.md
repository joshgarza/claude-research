---
date: 2026-02-24
topic: SkillsBench - benchmarking agent skills across diverse tasks
status: complete
tags: [llm-agents, benchmarking, agent-skills, procedural-knowledge, prompt-engineering]
---

# SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks

## Context

Research paper analysis from arXiv (2602.12670), submitted February 13, 2026. 40 authors led by Xiangyi Li, Wenbo Chen, Yimin Liu, and others. Licensed CC BY 4.0.

This paper introduces the first systematic benchmark for evaluating "Agent Skills" — structured packages of procedural knowledge (like SKILL.md files) that augment LLM agents. The motivation is that while the Skills ecosystem is growing (Anthropic's Agent Skills, MCP specifications), there has been no rigorous measurement of whether and when Skills actually help. The benchmark tests 84 tasks across 11 domains with 7,308 trajectories using 7 frontier models and 3 commercial agent harnesses.

## Findings

### What Are "Skills" in This Context

Skills are defined as structured procedural knowledge packages with four elements:
1. **Procedural content** — how-to guidance for accomplishing a task class
2. **Task-class applicability** — what kinds of tasks the Skill applies to
3. **Structured components** — a SKILL.md file plus supporting resources
4. **Portability** — can be shared and reused across agents/tasks

Explicitly excluded: system prompts, few-shot examples, RAG retrievals, and tool documentation. This is a narrow, well-defined scope.

### Benchmark Design

- **Scale:** 84 tasks across 11 domains, sourced from 322 candidate submissions by 105 contributors
- **Domains:** Healthcare, Manufacturing, Cybersecurity, Natural Science, Energy, Office/White Collar, Finance, Media/Content, Robotics, Mathematics, Software Engineering
- **Difficulty tiers:** Core (<60 min), Extended (1-4 hours), Extreme (>4 hours)
- **Verification:** Deterministic programmatic assertions (not LLM-as-judge)
- **Quality controls:** Automated structural/oracle validation, human review for validity/realism/anti-cheating, leakage audits to prevent Skills from encoding task-specific solutions

### Systems Evaluated

Seven frontier models tested across three commercial harnesses:
- **Models:** GPT-5.2, Claude Opus 4.5, Claude Opus 4.6, Claude Sonnet 4.5, Claude Haiku 4.5, Gemini 3 Pro, Gemini 3 Flash
- **Harnesses:** Claude Code (Anthropic), Gemini CLI (Google), Codex CLI (OpenAI)
- **Conditions:** No Skills, curated human-authored Skills, self-generated Skills

### Core Results

**Curated Skills provide a substantial average boost of +16.2 percentage points across all configurations.**

| Configuration | No Skills | With Skills | Delta |
|---|---|---|---|
| Gemini CLI + Flash | 31.3% | 48.7% | +17.4pp |
| Claude Code + Opus 4.5 | 22.0% | 45.3% | +23.3pp |
| Codex + GPT-5.2 | 30.6% | 44.7% | +14.1pp |
| Claude Code + Opus 4.6 | 30.6% | 44.5% | +13.9pp |
| Gemini CLI + Pro | 27.6% | 41.2% | +13.6pp |
| Claude Code + Sonnet 4.5 | 17.3% | 31.8% | +14.5pp |
| Claude Code + Haiku 4.5 | 11.0% | 27.7% | +16.7pp |

### Domain Variation Is Extreme

The most striking finding is how wildly Skills effectiveness varies by domain:

| Domain | With Skills | No Skills | Delta |
|---|---|---|---|
| Healthcare | 86.1% | 34.2% | **+51.9pp** |
| Manufacturing | 42.9% | 1.0% | **+41.9pp** |
| Cybersecurity | 44.0% | 20.8% | +23.2pp |
| Natural Science | 44.9% | 23.1% | +21.9pp |
| Energy | 47.5% | 29.5% | +17.9pp |
| Office & White Collar | 42.5% | 24.7% | +17.8pp |
| Finance | 27.6% | 12.5% | +15.1pp |
| Media & Content | 37.6% | 23.8% | +13.9pp |
| Robotics | 27.0% | 20.0% | +7.0pp |
| Mathematics | 47.3% | 41.3% | +6.0pp |
| Software Engineering | 38.9% | 34.4% | **+4.5pp** |

**Pattern:** Skills help most where the model has the largest procedural knowledge gap. Healthcare and Manufacturing see massive gains because models lack domain-specific procedural knowledge. Software Engineering and Mathematics see minimal gains because models already have strong procedural knowledge in those areas from training data.

### Self-Generated Skills Are Useless

This is arguably the most important finding: **self-generated Skills provide -1.3pp average effect** — essentially zero or slightly negative. Models cannot author the procedural knowledge they benefit from consuming.

Two failure modes identified:
1. **Identification without precision:** Models recognize domain knowledge is needed but generate vague procedures ("use pandas for data processing" instead of specific API patterns)
2. **Recognition failure:** On specialized domains (manufacturing, finance), models fail to identify the need for domain-specific Skills entirely, defaulting to general-purpose approaches

### Optimal Skills Design

Quantity matters, but with diminishing returns:
- 1 Skill: +17.8pp
- 2-3 Skills: +18.6pp (optimal sweet spot)
- 4+ Skills: +5.9pp (significant degradation)

Complexity has a non-obvious pattern:
- Detailed Skills: +18.8pp
- Compact Skills: +17.1pp
- Standard Skills: +10.1pp
- Comprehensive Skills: **-2.9pp** (actively harmful)

**Key insight: Focused, concise Skills outperform exhaustive documentation.** More is not better — too much procedural guidance degrades performance, likely due to context window pollution or attention dilution.

### Smaller Models + Skills Can Match Larger Models Without

Claude Haiku 4.5 with Skills (27.7%) approaches Claude Sonnet 4.5 without Skills (17.3%) — and actually exceeds it. This suggests Skills can partially compensate for model capacity limitations on procedural tasks, offering a cost-effective alternative to scaling up models.

### Harness Behavior Varies Significantly

- **Claude Code:** Highest Skills utilization rate; consistent improvements across all Claude models (+13.9pp to +23.3pp)
- **Gemini CLI:** Highest raw performance; solid improvements (+13.6pp to +17.4pp)
- **Codex CLI:** Frequently neglects provided Skills despite acknowledging them; agents implement solutions independently. This means the harness/scaffolding mediates whether Skills are even used.

### Cost Efficiency

Gemini 3 Flash demonstrates interesting cost-performance dynamics:
- Consumes 2.3x more input tokens than Gemini 3 Pro per task
- At API pricing ($0.50 vs $2.00 per 1M tokens), Flash is 44% cheaper per task ($0.55 vs $0.98)
- Achieves superior performance despite lower per-token cost

### Notable Task-Level Extremes

**Largest Skills benefits:**
- Mario-coin-counting: +85.7pp (2.9% → 88.6%)
- Sales-pivot-analysis: +85.7pp
- Flood-risk-analysis: +77.1pp
- SEC-financial-report: +74.3pp

**Tasks where Skills hurt (16 of 84 tasks showed degradation):**
- Taxonomy-tree-merge: -39.3pp
- Energy-AC-optimal-power-flow: -14.3pp
- Trend-anomaly-causal-inference: -12.9pp
- Exoplanet-detection-period: -11.4pp

This means Skills are not universally beneficial — roughly 19% of tasks showed negative effects.

## Limitations

1. **Terminal-only scope:** All tasks are containerized terminal-based tasks. Results may not generalize to GUI agents, multi-agent coordination, or extended-horizon workflows.
2. **Causal attribution unclear:** Adding Skills increases context length. A stronger baseline would be length-matched non-procedural context to isolate whether the benefit comes from procedural structure specifically or simply from more relevant context.
3. **Determinism concerns:** Containerization provides isolation but not perfect determinism. Multiple runs and paired comparisons mitigate but don't eliminate variance.
4. **No ecosystem-quality Skills tested:** All Skills were curated and high-quality. Real-world Skills ecosystems will contain varying quality levels — the benchmark doesn't test degraded-quality Skills.
5. **Single-turn evaluation:** Tasks are evaluated as single attempts, not iterative refinement workflows.

## Open Questions

1. **What makes a Skill harmful?** 16/84 tasks showed degraded performance with Skills. Is this predictable from task/Skill characteristics? Can we detect when to NOT apply a Skill?
2. **Can the self-generation gap be closed?** If models can't author effective Skills, who does? Is there a hybrid approach (model-generated + human-refined) that works?
3. **Length-matched baselines:** How much of the +16.2pp improvement is from procedural structure vs. simply having more domain-relevant context? This is the biggest methodological gap.
4. **Multi-modal Skills:** How do Skills perform for GUI-based agents or agents that process visual/audio inputs?
5. **Skills composition dynamics:** When multiple Skills interact or conflict, what happens? The 4+ Skills degradation suggests composition is non-trivial.
6. **Harness-mediated utilization:** Why does Codex CLI ignore Skills while Claude Code utilizes them? Is this a fundamental architectural difference or a tunable behavior?

## Extracted Principles

### Procedural Knowledge Augmentation
- **What:** Structured, focused procedural guidance (Skills) reliably improves agent performance, but only when the model has a genuine knowledge gap in the task domain.
- **Why:** Models already have strong procedural knowledge in well-represented training domains (SE, math). Skills add most value in specialized/underrepresented domains.
- **When:** Apply Skills for domain-specific tasks where the model lacks training-data coverage. Don't bother for software engineering or mathematical reasoning tasks.

### Less Is More for Agent Context
- **What:** 2-3 focused, detailed Skill modules outperform comprehensive documentation. Comprehensive Skills (-2.9pp) actively hurt performance.
- **Why:** Context window pollution and attention dilution. Models perform better with concise, targeted guidance than exhaustive references.
- **When:** Always prefer focused guidance over comprehensive docs when augmenting agents. This likely applies beyond Skills to system prompts, tool descriptions, and CLAUDE.md files.

### Self-Generated Procedural Knowledge Fails
- **What:** Models cannot reliably author the procedural knowledge they benefit from consuming. Self-generated Skills showed -1.3pp average effect.
- **Why:** Two failure modes — imprecise identification of needed knowledge, and failure to recognize specialized domain needs at all.
- **When:** Do not rely on agents to auto-generate their own Skills/instructions. Human-authored or human-curated procedural knowledge is currently necessary.

### Harness Architecture Mediates Tool Utilization
- **What:** The agent scaffolding/harness significantly affects whether Skills are utilized. Codex CLI frequently ignores Skills while Claude Code consistently uses them.
- **Why:** Different harnesses have different approaches to incorporating context into agent decision-making.
- **When:** When evaluating agent augmentation strategies, always test across multiple harnesses — a method that works in one may fail in another due to harness-level differences.

### Skills as Model Size Equalizer
- **What:** Smaller models with curated Skills can match or exceed larger models without Skills on procedural tasks.
- **Why:** Skills fill procedural knowledge gaps that scaling alone doesn't reliably address.
- **When:** Consider Skills-augmented smaller models as a cost-effective alternative to upgrading model size, especially for domain-specific applications.
