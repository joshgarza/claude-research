---
date: 2026-02-24
topic: evaluating-agents-md-context-files-for-coding-agents
status: complete
tags: [ai-agents, coding-agents, AGENTS.md, CLAUDE.md, context-files, SWE-bench, empirical-evaluation, software-engineering]
---

# Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents?

## Context

Research paper analysis from arxiv (arXiv:2602.11988, submitted 2026-02-12). Authors: Thibaud Gloaguen, Niels Mündler, Mark Müller, Veselin Raychev, Martin Vechev (ETH Zurich / DeepCode AI). License: CC-BY 4.0.

This paper is directly relevant to how we structure CLAUDE.md files in this repository and across projects. The industry trend of creating repository-level context files (AGENTS.md, CLAUDE.md, COPILOT.md, etc.) to guide AI coding agents has grown rapidly, but no rigorous empirical evaluation existed until this paper. The authors set out to answer a simple question: do these files actually help?

## Findings

### Core Problem

Developers increasingly create repository-level context files to guide LLM-based coding agents, following recommendations from tool vendors (Anthropic, OpenAI, etc.). These files typically contain project overviews, coding conventions, build instructions, and workflow requirements. The fundamental question — whether these files improve agent performance — had not been rigorously tested.

### Methodology

The authors designed a two-pronged evaluation:

**Benchmarks:**
- **SWE-bench Lite** (established): 300 tasks from 11 popular Python repositories, no developer-provided context files. Used to evaluate LLM-generated context files.
- **AGENTbench** (novel contribution): 138 instances from 12 Python repositories that actually contain developer-written context files. Sourced from real GitHub pull requests. Average codebase: 3,337 files; average context file: 9.7 sections.

**Agents tested:**
- Claude Code (with Sonnet 4.5)
- Codex (with GPT-5.2 and GPT-5.1 mini)
- Qwen Code (with Qwen3-30b-coder)

**Three context conditions:**
1. **None** — no context file provided
2. **LLM-generated** — auto-generated using each agent's recommended initialization commands
3. **Human-written** — developer-provided files (AGENTbench only)

### Key Results

**LLM-generated context files hurt performance:**
- SWE-bench Lite: 0.5% average decrease in success rate
- AGENTbench: 2% average decrease in success rate
- Negative impact in 5 of 8 model/dataset combinations
- Inference costs increase by 20-23% on average
- Agents take +2.45 to +3.92 additional steps per instance

**Developer-written context files are marginally better but still mixed:**
- 4% average improvement across agents (but no improvement for Claude Code specifically)
- Outperform LLM-generated files across all models
- Still increase costs up to 19%

**Specific cost example (Sonnet 4.5 on SWE-bench Lite):**
- Without context: $1.30/instance
- With LLM context: $1.51/instance (+16%)
- With human context: $1.30/instance (no change)

### Why Context Files Reduce Performance

The paper identifies three mechanisms:

1. **Agents follow instructions — that's the problem.** Tool usage analysis shows agents do comply with context file instructions (e.g., 1.6x increase in using `uv` when mentioned, 2.5x increase in repository-specific tooling). But compliance with unnecessary requirements makes tasks harder, not easier. Agents spend time following prescribed workflows that aren't relevant to the specific task at hand.

2. **Repository overviews don't help navigation.** Despite 95-100% of LLM-generated files including codebase overviews, agents discover relevant files at the same speed whether or not context exists (measured by steps to first interaction with modified files).

3. **Context files duplicate existing documentation.** When the authors removed all other documentation (.md files, docs/ folders), LLM-generated context files showed a 2.7% improvement — outperforming developer-written files. This confirms that in well-documented repositories, context files are largely redundant noise.

### Reasoning Cost Analysis

Models with extended reasoning (GPT-5.2, GPT-5.1 mini) spend significantly more reasoning tokens when context files are present:
- LLM-generated context: +22% and +14% reasoning tokens respectively
- Developer-provided context: +20% and +2% reasoning tokens
- Interpretation: the additional instructions make the tasks computationally harder for the model to reason about.

### Ablation Studies

- **Stronger models don't generate better context files.** Using GPT-5.2 to generate context for all agents: 2% improvement on SWE-bench Lite but 3% degradation on AGENTbench.
- **Prompt variation has minimal effect.** Comparing Codex vs. Claude Code generation prompts shows no consistent winner.

### Authors' Recommendations

**For LLM-generated context files:** Omit them entirely. They consistently increase costs without improving performance.

**For developer-written context files:** Include only minimal, essential requirements:
- Specific tooling that must be used (e.g., "use uv, not pip")
- Essential setup commands
- Hard constraints that would cause failures if violated

**Avoid:** Verbose codebase overviews, detailed architecture descriptions, and anything that duplicates existing documentation.

### Limitations Acknowledged

- **Python only.** Niche languages with less training data representation might benefit more from context files.
- **Task types limited.** Only issue resolution and feature addition tested; security, performance optimization, and code efficiency tasks unexplored.
- **Benchmark size.** AGENTbench has 138 instances (smaller than ideal).
- **Rapidly evolving field.** Agent capabilities improve quickly; results may shift with future models.

## Open Questions

1. **Do context files help more for non-Python languages?** Languages with less representation in training data might genuinely benefit from explicit guidance about idioms and tooling.

2. **Are there specific content types within context files that consistently help?** The paper treats files holistically — a finer-grained analysis of which sections help vs. hurt could inform better file design.

3. **How do results change for security and performance tasks?** These often require domain-specific knowledge that agents may lack; context files could be more valuable here.

4. **What about onboarding tasks vs. maintenance tasks?** Context files might be more useful for first-time interactions with a repository than for repeated work.

5. **Does this apply to context files used for non-coding tasks?** This project uses CLAUDE.md for research workflows, not coding — the dynamics may differ when the agent's task is exploratory rather than deterministic issue resolution.

6. **Could adaptive/task-specific context outperform static files?** Rather than one file for all tasks, context selected or generated based on the specific task might avoid the over-specification problem.

7. **How does context file length correlate with performance degradation?** The paper shows average 9.7 sections in developer files — would extremely short files (2-3 key rules) consistently help?

## Extracted Principles

### Less Is More for Agent Context Files
- **What:** Repository-level context files should contain only minimal, essential requirements — not comprehensive overviews.
- **Why:** Empirical evidence shows that additional instructions cause agents to follow unnecessary workflows, increasing cost (+20%) and reducing success rates. Agents are capable of navigating codebases on their own.
- **When:** Apply when writing CLAUDE.md, AGENTS.md, or similar files for any repository where AI coding agents will operate. Exception: may not apply to non-Python languages or highly unusual project structures.
- **Source:** arXiv:2602.11988 (this paper)

### LLM-Generated Context Files Are Counterproductive
- **What:** Do not use agent initialization commands (like `claude init` or similar) to auto-generate context files. These files consistently degrade performance while increasing costs.
- **Why:** LLM-generated files duplicate existing documentation and add unnecessary requirements. They make tasks harder for agents without aiding navigation.
- **When:** Always — until tooling improves. Developer-written minimal files are strictly better than LLM-generated comprehensive ones.
- **Source:** arXiv:2602.11988

### Agent Compliance Is Not the Bottleneck
- **What:** Modern coding agents reliably follow instructions in context files. The problem is not compliance but over-specification — giving instructions that aren't helpful for the task at hand.
- **Why:** Tool usage data shows 1.6-2.5x increases in prescribed tool usage when mentioned in context files. Agents listen; the issue is what you tell them.
- **When:** When designing any form of agent instruction, focus on "what must not be violated" rather than "what should ideally be done."
- **Source:** arXiv:2602.11988

### Context Files Add Value in Documentation Deserts
- **What:** When no other documentation exists, even LLM-generated context files provide a measurable (+2.7%) improvement.
- **Why:** In the absence of READMEs, docstrings, and docs folders, context files become the only source of project-level guidance.
- **When:** For undocumented or poorly documented repositories. If your project already has good docs, a context file adds noise rather than signal.
- **Source:** arXiv:2602.11988
